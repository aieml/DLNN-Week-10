{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym\n",
    "import gym\n",
    "\n",
    "env=gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "\n",
    "print(action_space_size,state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q_table=np.zeros((state_space_size,action_space_size))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.0 True Down\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#testing the environment\n",
    "from IPython.display import clear_output\n",
    "import time as t\n",
    "\n",
    "done=False\n",
    "\n",
    "env.reset()\n",
    "\n",
    "action_dict={0:'Left',1:'Down',2:'Right',3:'Up'}\n",
    "\n",
    "while(not done):\n",
    "    \n",
    "    action=np.random.randint(0,4,1)[0]\n",
    "    new_state,reward,done,info=env.step(action)\n",
    "    print(new_state,reward,done,action_dict[action])\n",
    "    env.render()\n",
    "    t.sleep(3)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100000\n",
    "max_steps_per_episodes=100\n",
    "\n",
    "learning_rate=0.1\n",
    "discount_rate=0.99\n",
    "\n",
    "exploration_rate=1\n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rewards=[]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    current_reward=0\n",
    "    \n",
    "    while(not done):\n",
    "        \n",
    "        exploaration_rate_threshold=random.uniform(0,1)\n",
    "        \n",
    "        if(exploaration_rate_threshold>exploration_rate):\n",
    "            \n",
    "            action=np.argmax(Q_table[state,:])   #exploitation\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action=env.action_space.sample()    #exploration\n",
    "        \n",
    "        #perform the action\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #updating the Q\n",
    "        Q_table[state,action]=(Q_table[state,action]*(1-learning_rate))+(reward+discount_rate*np.max(Q_table[new_state,:]))*learning_rate  \n",
    "        \n",
    "        #state change\n",
    "        state=new_state\n",
    "        \n",
    "        current_reward=current_reward+reward\n",
    "        \n",
    "        if(done==True):\n",
    "            break\n",
    "    \n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
    "    rewards.append(current_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================Q Table======================\n",
      " [[0.53203982 0.5022373  0.49598616 0.49822111]\n",
      " [0.38386325 0.24712009 0.31090362 0.51393418]\n",
      " [0.39091918 0.41306361 0.40432606 0.47892859]\n",
      " [0.34248827 0.34240872 0.30979709 0.45069393]\n",
      " [0.54238296 0.43212603 0.39912397 0.34367376]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30557414 0.16368169 0.14597505 0.11991735]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3786673  0.37253586 0.46255245 0.57927813]\n",
      " [0.37523086 0.63270485 0.38389203 0.44598003]\n",
      " [0.59519845 0.33144866 0.36167184 0.3956302 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3672431  0.6101612  0.72158659 0.4491779 ]\n",
      " [0.71686739 0.84228796 0.72499297 0.73801185]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('======================Q Table======================\\n',Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -1000, average reward: 0.142\n",
      "2 -1000, average reward: 0.188\n",
      "3 -1000, average reward: 0.209\n",
      "4 -1000, average reward: 0.177\n",
      "5 -1000, average reward: 0.191\n",
      "6 -1000, average reward: 0.454\n",
      "7 -1000, average reward: 0.643\n",
      "8 -1000, average reward: 0.676\n",
      "9 -1000, average reward: 0.679\n",
      "10 -1000, average reward: 0.655\n",
      "11 -1000, average reward: 0.684\n",
      "12 -1000, average reward: 0.645\n",
      "13 -1000, average reward: 0.656\n",
      "14 -1000, average reward: 0.659\n",
      "15 -1000, average reward: 0.697\n",
      "16 -1000, average reward: 0.655\n",
      "17 -1000, average reward: 0.671\n",
      "18 -1000, average reward: 0.693\n",
      "19 -1000, average reward: 0.663\n",
      "20 -1000, average reward: 0.703\n",
      "21 -1000, average reward: 0.722\n",
      "22 -1000, average reward: 0.637\n",
      "23 -1000, average reward: 0.653\n",
      "24 -1000, average reward: 0.689\n",
      "25 -1000, average reward: 0.696\n",
      "26 -1000, average reward: 0.691\n",
      "27 -1000, average reward: 0.635\n",
      "28 -1000, average reward: 0.676\n",
      "29 -1000, average reward: 0.681\n",
      "30 -1000, average reward: 0.638\n",
      "31 -1000, average reward: 0.703\n",
      "32 -1000, average reward: 0.672\n",
      "33 -1000, average reward: 0.664\n",
      "34 -1000, average reward: 0.681\n",
      "35 -1000, average reward: 0.701\n",
      "36 -1000, average reward: 0.669\n",
      "37 -1000, average reward: 0.667\n",
      "38 -1000, average reward: 0.708\n",
      "39 -1000, average reward: 0.694\n",
      "40 -1000, average reward: 0.688\n",
      "41 -1000, average reward: 0.662\n",
      "42 -1000, average reward: 0.659\n",
      "43 -1000, average reward: 0.661\n",
      "44 -1000, average reward: 0.676\n",
      "45 -1000, average reward: 0.69\n",
      "46 -1000, average reward: 0.672\n",
      "47 -1000, average reward: 0.683\n",
      "48 -1000, average reward: 0.667\n",
      "49 -1000, average reward: 0.675\n",
      "50 -1000, average reward: 0.656\n",
      "51 -1000, average reward: 0.702\n",
      "52 -1000, average reward: 0.66\n",
      "53 -1000, average reward: 0.69\n",
      "54 -1000, average reward: 0.694\n",
      "55 -1000, average reward: 0.678\n",
      "56 -1000, average reward: 0.699\n",
      "57 -1000, average reward: 0.684\n",
      "58 -1000, average reward: 0.662\n",
      "59 -1000, average reward: 0.711\n",
      "60 -1000, average reward: 0.671\n",
      "61 -1000, average reward: 0.654\n",
      "62 -1000, average reward: 0.684\n",
      "63 -1000, average reward: 0.678\n",
      "64 -1000, average reward: 0.664\n",
      "65 -1000, average reward: 0.716\n",
      "66 -1000, average reward: 0.726\n",
      "67 -1000, average reward: 0.681\n",
      "68 -1000, average reward: 0.692\n",
      "69 -1000, average reward: 0.685\n",
      "70 -1000, average reward: 0.678\n",
      "71 -1000, average reward: 0.702\n",
      "72 -1000, average reward: 0.684\n",
      "73 -1000, average reward: 0.679\n",
      "74 -1000, average reward: 0.675\n",
      "75 -1000, average reward: 0.688\n",
      "76 -1000, average reward: 0.688\n",
      "77 -1000, average reward: 0.697\n",
      "78 -1000, average reward: 0.698\n",
      "79 -1000, average reward: 0.666\n",
      "80 -1000, average reward: 0.687\n",
      "81 -1000, average reward: 0.678\n",
      "82 -1000, average reward: 0.697\n",
      "83 -1000, average reward: 0.66\n",
      "84 -1000, average reward: 0.696\n",
      "85 -1000, average reward: 0.683\n",
      "86 -1000, average reward: 0.688\n",
      "87 -1000, average reward: 0.672\n",
      "88 -1000, average reward: 0.677\n",
      "89 -1000, average reward: 0.696\n",
      "90 -1000, average reward: 0.672\n",
      "91 -1000, average reward: 0.711\n",
      "92 -1000, average reward: 0.682\n",
      "93 -1000, average reward: 0.684\n",
      "94 -1000, average reward: 0.671\n",
      "95 -1000, average reward: 0.681\n",
      "96 -1000, average reward: 0.663\n",
      "97 -1000, average reward: 0.687\n",
      "98 -1000, average reward: 0.673\n",
      "99 -1000, average reward: 0.685\n",
      "100 -1000, average reward: 0.683\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes=np.split(np.array(rewards),num_episodes/1000)\n",
    "\n",
    "for i,array in enumerate(rewards_per_thousand_episodes):\n",
    "    print(i+1,'-1000, average reward:',np.mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.0 True Left\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    print(\"Episode :\",episode)\n",
    "\n",
    "    done=False\n",
    "\n",
    "    state=env.reset()\n",
    "\n",
    "    action_dict={0:'Left',1:'Down',2:'Right',3:'Up'}\n",
    "\n",
    "    while(not done):\n",
    "\n",
    "        action=np.argmax(Q_table[state,:])\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        print(new_state,reward,done,action_dict[action])\n",
    "        env.render()\n",
    "        t.sleep(0.5)\n",
    "        clear_output(wait=True)\n",
    "        state=new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
