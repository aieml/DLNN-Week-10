{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "print(action_space_size,state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Q_table=np.zeros((state_space_size,action_space_size))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100000\n",
    "max_steps_per_episodes=100\n",
    "\n",
    "learning_rate=0.1\n",
    "discount_rate=0.99\n",
    "\n",
    "exploration_rate=1\n",
    "max_exploration_rate=1\n",
    "min_exploration_rate=0.01\n",
    "exploration_decay_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    state=env.reset() #reset the env to starting point\n",
    "    \n",
    "    done=False\n",
    "    reward_current_episode=0 #to save rewards of the current epiosode\n",
    "    \n",
    "    for step in range(max_steps_per_episodes):\n",
    "        \n",
    "        exploaration_rate_threshold=random.uniform(0,1)\n",
    "        \n",
    "        if(exploaration_rate_threshold>exploration_rate):\n",
    "            \n",
    "            action=np.argmax(Q_table[state,:])\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            action=env.action_space.sample()\n",
    "        \n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        #updating the Q\n",
    "        Q_table[state,action]=(Q_table[state,action]*(1-learning_rate))+(reward+discount_rate*np.max(Q_table[new_state,:]))*learning_rate\n",
    "        \n",
    "        state=new_state\n",
    "        \n",
    "        reward_current_episode=reward_current_episode+reward\n",
    "        \n",
    "        if(done==True):\n",
    "            break\n",
    "            \n",
    "    #exploration decay\n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
    "    rewards.append(reward_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================Q Table======================\n",
      " [[0.53668642 0.49086014 0.48498933 0.48025646]\n",
      " [0.34907004 0.35234319 0.34725324 0.48844795]\n",
      " [0.4142586  0.40951092 0.40162504 0.45898447]\n",
      " [0.39707226 0.29602548 0.28766559 0.4350137 ]\n",
      " [0.55443244 0.29959559 0.29542422 0.30066851]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33320201 0.15676945 0.21076191 0.11425949]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36859706 0.44952389 0.40262537 0.58194983]\n",
      " [0.49437416 0.62382049 0.27333371 0.462842  ]\n",
      " [0.61704336 0.41650256 0.47249309 0.33894911]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.54816808 0.59319805 0.70846012 0.53488001]\n",
      " [0.75799384 0.85357284 0.76182558 0.72879085]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('======================Q Table======================\\n',Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -1000, average reward: 0.494\n",
      "2 -1000, average reward: 0.693\n",
      "3 -1000, average reward: 0.693\n",
      "4 -1000, average reward: 0.687\n",
      "5 -1000, average reward: 0.699\n",
      "6 -1000, average reward: 0.665\n",
      "7 -1000, average reward: 0.712\n",
      "8 -1000, average reward: 0.658\n",
      "9 -1000, average reward: 0.692\n",
      "10 -1000, average reward: 0.7\n",
      "11 -1000, average reward: 0.687\n",
      "12 -1000, average reward: 0.722\n",
      "13 -1000, average reward: 0.687\n",
      "14 -1000, average reward: 0.674\n",
      "15 -1000, average reward: 0.69\n",
      "16 -1000, average reward: 0.691\n",
      "17 -1000, average reward: 0.72\n",
      "18 -1000, average reward: 0.67\n",
      "19 -1000, average reward: 0.693\n",
      "20 -1000, average reward: 0.681\n",
      "21 -1000, average reward: 0.689\n",
      "22 -1000, average reward: 0.674\n",
      "23 -1000, average reward: 0.662\n",
      "24 -1000, average reward: 0.694\n",
      "25 -1000, average reward: 0.716\n",
      "26 -1000, average reward: 0.699\n",
      "27 -1000, average reward: 0.675\n",
      "28 -1000, average reward: 0.68\n",
      "29 -1000, average reward: 0.673\n",
      "30 -1000, average reward: 0.671\n",
      "31 -1000, average reward: 0.674\n",
      "32 -1000, average reward: 0.664\n",
      "33 -1000, average reward: 0.684\n",
      "34 -1000, average reward: 0.688\n",
      "35 -1000, average reward: 0.676\n",
      "36 -1000, average reward: 0.668\n",
      "37 -1000, average reward: 0.704\n",
      "38 -1000, average reward: 0.689\n",
      "39 -1000, average reward: 0.676\n",
      "40 -1000, average reward: 0.707\n",
      "41 -1000, average reward: 0.706\n",
      "42 -1000, average reward: 0.683\n",
      "43 -1000, average reward: 0.678\n",
      "44 -1000, average reward: 0.666\n",
      "45 -1000, average reward: 0.655\n",
      "46 -1000, average reward: 0.68\n",
      "47 -1000, average reward: 0.661\n",
      "48 -1000, average reward: 0.666\n",
      "49 -1000, average reward: 0.688\n",
      "50 -1000, average reward: 0.714\n",
      "51 -1000, average reward: 0.635\n",
      "52 -1000, average reward: 0.687\n",
      "53 -1000, average reward: 0.677\n",
      "54 -1000, average reward: 0.676\n",
      "55 -1000, average reward: 0.654\n",
      "56 -1000, average reward: 0.675\n",
      "57 -1000, average reward: 0.689\n",
      "58 -1000, average reward: 0.7\n",
      "59 -1000, average reward: 0.705\n",
      "60 -1000, average reward: 0.686\n",
      "61 -1000, average reward: 0.687\n",
      "62 -1000, average reward: 0.693\n",
      "63 -1000, average reward: 0.711\n",
      "64 -1000, average reward: 0.679\n",
      "65 -1000, average reward: 0.697\n",
      "66 -1000, average reward: 0.682\n",
      "67 -1000, average reward: 0.686\n",
      "68 -1000, average reward: 0.708\n",
      "69 -1000, average reward: 0.644\n",
      "70 -1000, average reward: 0.688\n",
      "71 -1000, average reward: 0.701\n",
      "72 -1000, average reward: 0.712\n",
      "73 -1000, average reward: 0.667\n",
      "74 -1000, average reward: 0.673\n",
      "75 -1000, average reward: 0.69\n",
      "76 -1000, average reward: 0.681\n",
      "77 -1000, average reward: 0.684\n",
      "78 -1000, average reward: 0.696\n",
      "79 -1000, average reward: 0.682\n",
      "80 -1000, average reward: 0.694\n",
      "81 -1000, average reward: 0.693\n",
      "82 -1000, average reward: 0.655\n",
      "83 -1000, average reward: 0.641\n",
      "84 -1000, average reward: 0.689\n",
      "85 -1000, average reward: 0.676\n",
      "86 -1000, average reward: 0.689\n",
      "87 -1000, average reward: 0.662\n",
      "88 -1000, average reward: 0.705\n",
      "89 -1000, average reward: 0.699\n",
      "90 -1000, average reward: 0.667\n",
      "91 -1000, average reward: 0.697\n",
      "92 -1000, average reward: 0.675\n",
      "93 -1000, average reward: 0.687\n",
      "94 -1000, average reward: 0.68\n",
      "95 -1000, average reward: 0.694\n",
      "96 -1000, average reward: 0.695\n",
      "97 -1000, average reward: 0.682\n",
      "98 -1000, average reward: 0.689\n",
      "99 -1000, average reward: 0.668\n",
      "100 -1000, average reward: 0.662\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thousand_episodes=np.split(np.array(rewards),num_episodes/1000)\n",
    "\n",
    "for i,array in enumerate(rewards_per_thousand_episodes):\n",
    "    print(i+1,'-1000, average reward:',np.mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "======Goal Reached======\n"
     ]
    }
   ],
   "source": [
    "#Playing the game using trained Q values\n",
    "\n",
    "for episode in range(5):\n",
    "    \n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    print('============Episode:',episode+1,'============')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episodes):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action=np.argmax(Q_table[state,:])\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        \n",
    "        if done==True:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            \n",
    "            if reward==1:\n",
    "                print('======Goal Reached======')\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('======Hole Reached======')\n",
    "                time.sleep(3)\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state=new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
